from llama_index.core import SimpleDirectoryReader, ServiceContext, StorageContext, VectorStoreIndex
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from transformers import BitsAndBytesConfig, AutoTokenizer
import chromadb
import torch
import gradio as gr
from gradio.themes.base import Base

def initialize_chroma_client(collection_name: str) -> ChromaVectorStore:
    chroma_client = chromadb.PersistentClient()
    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    return ChromaVectorStore(chroma_collection=chroma_collection)

def load_documents(directory: str):
    reader = SimpleDirectoryReader(directory)
    return reader.load_data()

def initialize_llm(system_prompt: str, query_wrapper_prompt: str) -> HuggingFaceLLM:
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    llm = HuggingFaceLLM(
        context_window=2000,
        max_new_tokens=256,
        generate_kwargs={"temperature": 0.5, "do_sample": True},
        system_prompt=system_prompt,
        query_wrapper_prompt=query_wrapper_prompt,
        model_kwargs={"quantization_config": quantization_config, "torch_dtype": torch.float16},
        tokenizer_name="mistralai/Mistral-7B-Instruct-v0.2",
        model_name="mistralai/Mistral-7B-Instruct-v0.2",
        device_map="auto",
        tokenizer_kwargs={"max_length": 2000}
    )
    return llm

def create_query_engine():
    chroma_collection_name = 'test_collection'
    document_directory = 'data'
    system_prompt = "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided."
    query_wrapper_prompt = "{query_str}"

    vector_store = initialize_chroma_client(chroma_collection_name)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    documents = load_documents(document_directory)
    llm = initialize_llm(system_prompt, query_wrapper_prompt)
    embed_model = HuggingFaceEmbedding(model_name="thenlper/gte-base")
    service_context = ServiceContext.from_defaults(
        chunk_size=512,
        llm=llm,
        embed_model=embed_model,
        chunk_overlap=200
    )
    index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)
    
    return index.as_query_engine(similarity_top_k=3)

def query_model(question: str) -> str:
    response = query_engine.query(question)
    return str(response)

if __name__ == "__main__":
    query_engine = create_query_engine()


    with gr.Blocks(theme=Base(), title="Question Answering My Documents + RAG") as demo:
        gr.Markdown(
            """
            # Question Answering App using Vector Search + RAG Architecture
            """
        )
        textbox = gr.Textbox(label="Enter your question:")
        with gr.Row():
            temperature = gr.Slider(0, 1.5, 0.5, step=0.1, label="Temperature")
            k_docs = gr.Slider(1, 10, 4, step=1, label="Number of documents to return")
        with gr.Row():
            button = gr.Button("Submit", variant="primary")
        with gr.Column():
            output1 = gr.Textbox(lines=1, max_lines=10, label="Output generated by Mistral-7B")
            #output2 = gr.Textbox(lines=1, max_lines=10, label="Output generated by chaining Vector Search to Langchain's RetrieverQA + OpenAI LLM")

        button.click(query_model, inputs=[textbox], outputs=[output1])

    demo.launch()
