from llama_index.core import SimpleDirectoryReader, ServiceContext, StorageContext, VectorStoreIndex
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
#from llama_index.embeddings import LangchainEmbedding
#from llama_index.langchain_helpers.text_splitter import SentenceSplitter
from transformers import BitsAndBytesConfig, AutoTokenizer
import chromadb
import torch
import gradio as gr
from gradio.themes.base import Base

def initialize_chroma_client(collection_name: str) -> ChromaVectorStore:
    chroma_client = chromadb.PersistentClient()
    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    return ChromaVectorStore(chroma_collection=chroma_collection)

def load_documents(directory: str):
    reader = SimpleDirectoryReader(directory)
    return reader.load_data()

def initialize_llm(system_prompt: str, query_wrapper_prompt: str) -> HuggingFaceLLM:
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    llm = HuggingFaceLLM(
        context_window=2000,
        max_new_tokens=256,
        generate_kwargs={"do_sample": True, "top_p": 0.95, "top_k": 50, "temperature": 0.8, "num_beams":1},
        system_prompt=system_prompt,
        query_wrapper_prompt=query_wrapper_prompt,
        model_kwargs={"quantization_config": quantization_config, "torch_dtype": torch.float16},
        tokenizer_name="mistralai/Mistral-7B-Instruct-v0.2",
        model_name="mistralai/Mistral-7B-Instruct-v0.2",
        device_map="auto",
        tokenizer_kwargs={"max_length": 2000}
        
    )
    return llm

def create_query_engine():
    chroma_collection_name = 'test_collection'
    document_directory = 'data'
    system_prompt = "You are a chatbot, able to have normal interactions, as well as talk about any document available."
    query_wrapper_prompt = "{query_str}"

    vector_store = initialize_chroma_client(chroma_collection_name)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    documents = load_documents(document_directory)
    #prompt_helper = PromptHelper.from_llm_metadata(llm_metadata=llm.metadata)
    llm = initialize_llm(system_prompt, query_wrapper_prompt)
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-mpnet-base-v2")
    #embed_model = LangchainEmbedding(langchain_embeddings=HuggingFaceEmbedding(
    #    model_name="sentence-transformers/all-mpnet-base-v2",
    #model_kwargs={'device': 'cuda:1'},
    #encode_kwargs={'normalize_embeddings': False}
    #))
    service_context = ServiceContext.from_defaults(
        chunk_size=512,
        #prompt_helper=prompt_helper,
        llm=llm,
        embed_model=embed_model,
        chunk_overlap=200
    )
    index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)
    #memory = ChatMemoryBuffer.from_defaults(token_limit=1500)
    return index.as_chat_engine(chat_mode='context',retriever_mode='embedding', similarity_top_k=3)

def query_model(question: str) -> str:
    response = query_engine.chat(question)
    return str(response)


if __name__ == "__main__":
    query_engine = create_query_engine()

    with gr.Blocks(theme=Base(), title="Chat My Documents + RAG") as demo:
        gr.Markdown(
            """
            # ChatBot App using Vector Search + RAG Architecture
            """
        )
        textbox = gr.Textbox(label="Write a message:")
        # with gr.Row():
        #     temperature = gr.Slider(0, 1.5, 0.5, step=0.1, label="Temperature")
        #     k_docs = gr.Slider(1, 10, 4, step=1, label="Number of documents to return")
        with gr.Row():
            button = gr.Button("Submit", variant="primary")
        with gr.Column():
            output1 = gr.Textbox(lines=1, max_lines=10, label="Output generated by Mistral-7B")
            #output2 = gr.Textbox(lines=1, max_lines=10, label="Output generated by chaining Vector Search to Langchain's RetrieverQA + OpenAI LLM")

        button.click(query_model, inputs=[textbox], outputs=[output1])

    demo.launch()